predictor/gin.singleton.constructor = @ChempropBinaryClassifier
predictor = @predictor/gin.singleton()

#==========================================================================================================#
#   MODEL PARAMETERS
#==========================================================================================================#
#   If the parameter below is set to false, the script will not execute hyperparameter optimization step.
#   Instead, the model will be trained using fixed hyperparameters provided in params dict.

    ChempropBinaryClassifier.optimize_hyperparameters = False

    ChempropBinaryClassifier.params = {
        'mp_type': 'atom',          #   Type of the message passing layer. Can be 'atom' or 'bond'.
        'mp_hidden_dim': 1000,      #   Hidden dimension of the message passing layer
        'mp_num_layers': 2,         #   Number of message passing layers
        'agg_type': 'mean',         #   Type of the aggregation function. Can be 'mean', 'sum', or 'norm'
        'ffn_hidden_dim': 1000,     #   Size of the FFN hidden layer
        'ffn_num_layers': 2,        #   Number of FFN hidden layers
        'batch_norm': True,         #   Whether to use batch normalization
    }

#==========================================================================================================#
#   If optimize_hyperparameters is set to True, the training script will first optimize the values of
#   hyperparameters using random search - CV strategy

    ChempropBinaryClassifier.optimization_iterations = 2     # maximum times the script is alowed to draw and
                                                        # evaluate a new set of hyperparameter values

    ChempropBinaryClassifier.n_workers = 8                      # number of CPUs to use

#   Dictionary of distributions to sample hyperparameter values from. To each of the models' hyperparameters
#   either a discreete list or a continous distribution may be assigned. Below is a brief demonstration on
#   how to configure continuous probability distributions for the hyperparameters.

#   There are four distributions you can use, all parametrized by min and max:
#       * Uniform       (continuous)
#       * LogUniform    (continuous)
#       * QUniform      (discrete)
#       * QLogUniform   (discrete)

#   If you wanted to provide a LogUniform distribution for the parameter C, remember to put it in
#   some scope (ex. @C/LogUniform, where C/ is the scope). That way, when setting C/LogUniform.min
#   and C/LogUniform.max distribution parameters later on, those will only change for our C hyperparam
#   distribution, and not for any other LogNormals used in this config. The @ is essential when passing
#   the distribution itself (which is a class) to the dictionary, but not needed while setting attributes.

    ChempropBinaryClassifier.params_distribution = {
        'mp_type': ['atom'],
        'mp_hidden_dim': @mp_hidden_dim/QLogUniform(),
        'mp_num_layers': [1, 2, 3],
        'agg_type': ['mean', 'sum', 'norm'],
        'ffn_hidden_dim': @ffn_hidden_dim/QLogUniform(),
        'ffn_num_layers': [1, 2, 3],
        'batch_norm': [True, False]
    }

#   Parametrizing the distribution functions:

    ffn_hidden_dim/QLogUniform.min = 10
    ffn_hidden_dim/QLogUniform.max = 1000

    mp_hidden_dim/QLogUniform.min = 10
    mp_hidden_dim/QLogUniform.max = 1000

#==========================================================================================================#
#   OTHER
#==========================================================================================================#
#   Number of epochs to train the model
    ChempropBinaryClassifier.epochs = 10

#   Whether to use GPU for training
    ChempropBinaryClassifier.use_gpu = True
#==========================================================================================================#