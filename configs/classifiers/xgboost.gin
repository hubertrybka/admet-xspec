predictor/gin.singleton.constructor = @XGBoostClassifier
predictor = @predictor/gin.singleton()

#==========================================================================================================#
#   If the parameter below is set to false, the script will not execute hyperparameter optimization step.
#   Instead, the model will be trained using fixed hyperparameters provided in params dict.

    XGBoostClassifier.optimize_hyperparameters = True

    XGBoostClassifier.params = {
        'does_not_work': 'yet'
        }

#==========================================================================================================#
#   Define the target metric, one to be optimized during hyperparameter search.
#   Supported metrics are: 'accuracy', 'roc_auc', 'f1', 'recision', 'recall'

    XGBoostClassifier.target_metric = 'roc_auc'

#==========================================================================================================#
#   If optimize_hyperparameters is set to True, the training script will first optimize the values of
#   hyperparameters using random search - CV strategy

    XGBoostClassifier.optimization_iterations = 200    # maximum times the script is allowed to draw and
                                                        # evaluate a new set of hyperparameter values

    XGBoostClassifier.n_jobs = 20                      # number of CPUs to use
    XGBoostClassifier.n_folds = 5                     # number of cross-validation folds

#   Dictionary of distributions to sample hyperparameter values from. To each of the models' hyperparameters
#   either a discrete list or a continuous distribution may be assigned. Below is a brief demonstration on
#   how to configure probability distributions for the hyperparameters.

#   There are four distributions you can use, all parametrized by min and max:
#       * Uniform       (continuous)
#       * LogUniform    (continuous)
#       * QUniform      (discrete)
#       * QLogUniform   (discrete)

#   If you wanted to provide a LogUniform distribution for the parameter n_estimators, remember to put it in
#   some scope (ex. @n_estimators/LogUniform(), where n_estimators/ is the scope). That way, when setting
#   n_estimators/LogUniform.min and n_estimators/LogUniform.max distribution parameters later on, those will
#   only change for our 'n_estimators' hyperparam distribution, and not for any other LogUniforms used in this
#   config. The @ is essential when passing the distribution itself (which is an instance of a class) to the
#   params_distribution dict, but not when setting the distribution parameters.

    XGBoostClassifier.params_distribution = {
        'n_estimators': @n_estimators/QLogUniform(),
        'criterion': ['gini', 'entropy'],
        'max_depth': @max_depth/QUniform(),
        'min_samples_split': @min_samples_split/QUniform(),
        'min_samples_leaf': @min_samples_leaf/QUniform(),
        'max_features': ['sqrt', 'log2']
        }

#   Parametrizing the distribution functions:

        n_estimators/QLogUniform.min = 100
        n_estimators/QLogUniform.max = 2000

        max_depth/QUniform.min = 2
        max_depth/QUniform.max = 50

        min_samples_split/QUniform.min = 2
        min_samples_split/QUniform.max = 10

        min_samples_leaf/QUniform.min = 1
        min_samples_leaf/QUniform.max = 10

#==========================================================================================================#
